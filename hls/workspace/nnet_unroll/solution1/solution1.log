==============================================================
File generated by Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC
Version: 2017.4.1
Copyright (C) 1986-2018 Xilinx, Inc. All Rights Reserved.

==============================================================

INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xc7z020clg400-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Analyzing design file 'nnet_unroll/solution1/nnet.cpp' ...
INFO: [HLS 200-10] Validating synthesis directives ...
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:02:42 ; elapsed = 00:01:52 . Memory (MB): peak = 361.434 ; gain = 13.375 ; free physical = 832 ; free virtual = 3353
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:02:56 ; elapsed = 00:02:06 . Memory (MB): peak = 361.434 ; gain = 13.375 ; free physical = 800 ; free virtual = 3352
INFO: [HLS 200-10] Starting code transformations ...
INFO: [XFORM 203-501] Unrolling loop 'conv_layer1_label0' (nnet_unroll/solution1/nnet.cpp:28) in function 'conv_layer1(ap_fixed<24, 4, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<24, 4, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<24, 4, (ap_q_mode)5, (ap_o_mode)3, 0> (*) [4][1][8], ap_fixed<24, 4, (ap_q_mode)5, (ap_o_mode)3, 0>*)' completely.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:04:02 ; elapsed = 00:03:14 . Memory (MB): peak = 825.434 ; gain = 477.375 ; free physical = 376 ; free virtual = 3029
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'relu' into 'conv_layer1' (nnet_unroll/solution1/nnet.cpp:29) automatically.
INFO: [XFORM 203-602] Inlining function 'relu' into 'conv_layer2' (nnet_unroll/solution1/nnet.cpp:52) automatically.
INFO: [XFORM 203-602] Inlining function 'relu' into 'fc_layer1' (nnet_unroll/solution1/nnet.cpp:117) automatically.
INFO: [XFORM 203-602] Inlining function 'relu' into 'fc_layer2' (nnet_unroll/solution1/nnet.cpp:131) automatically.
INFO: [XFORM 203-602] Inlining function 'relu' into 'fc_layer3' (nnet_unroll/solution1/nnet.cpp:145) automatically.
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:04:04 ; elapsed = 00:03:16 . Memory (MB): peak = 827.062 ; gain = 479.004 ; free physical = 359 ; free virtual = 3017
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'pool_layer2_label15' (nnet_unroll/solution1/nnet.cpp:84) in function 'pool_layer2' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'conv_layer2_label5' (nnet_unroll/solution1/nnet.cpp:44) in function 'conv_layer2' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'pool_layer1_label11' (nnet_unroll/solution1/nnet.cpp:64) in function 'pool_layer1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'conv_layer1_label3' (nnet_unroll/solution1/nnet.cpp:21) in function 'conv_layer1' for pipelining.
INFO: [XFORM 203-501] Unrolling loop 'pool_layer2_label14' (nnet_unroll/solution1/nnet.cpp:85) in function 'pool_layer2' completely.
INFO: [XFORM 203-501] Unrolling loop 'pool_layer2_label13' (nnet_unroll/solution1/nnet.cpp:88) in function 'pool_layer2' completely.
INFO: [XFORM 203-501] Unrolling loop 'pool_layer2_label12' (nnet_unroll/solution1/nnet.cpp:89) in function 'pool_layer2' completely.
INFO: [XFORM 203-501] Unrolling loop 'conv_layer2_label7' (nnet_unroll/solution1/nnet.cpp:49) in function 'conv_layer2' completely.
INFO: [XFORM 203-501] Unrolling loop 'conv_layer2_label6' (nnet_unroll/solution1/nnet.cpp:50) in function 'conv_layer2' completely.
INFO: [XFORM 203-501] Unrolling loop 'conv_layer2_label4' (nnet_unroll/solution1/nnet.cpp:51) in function 'conv_layer2' completely.
INFO: [XFORM 203-501] Unrolling loop 'pool_layer1_label10' (nnet_unroll/solution1/nnet.cpp:65) in function 'pool_layer1' completely.
INFO: [XFORM 203-501] Unrolling loop 'pool_layer1_label9' (nnet_unroll/solution1/nnet.cpp:68) in function 'pool_layer1' completely.
INFO: [XFORM 203-501] Unrolling loop 'pool_layer1_label8' (nnet_unroll/solution1/nnet.cpp:69) in function 'pool_layer1' completely.
INFO: [XFORM 203-501] Unrolling loop 'conv_layer1_label2' (nnet_unroll/solution1/nnet.cpp:26) in function 'conv_layer1' completely.
INFO: [XFORM 203-501] Unrolling loop 'conv_layer1_label1' (nnet_unroll/solution1/nnet.cpp:27) in function 'conv_layer1' completely.
INFO: [XFORM 203-102] Partitioning array 'weight.V.0' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.3' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.0.0' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.0.1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.0.2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.0.3' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.1.0' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.1.1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.1.2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.1.3' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.2.0' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.2.1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.2.2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.2.3' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.3.0' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.3.1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.3.2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.3.3' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.0' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.0.0' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.0.1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.1.0' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'weight.V.1.1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer2_weights.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer2_weights.V.0' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer2_weights.V.0.0' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer2_weights.V.0.1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer2_weights.V.1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer2_weights.V.1.0' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer2_weights.V.1.1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer1_weights.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer1_weights.V.0' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer1_weights.V.0.0' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer1_weights.V.0.1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer1_weights.V.0.2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer1_weights.V.0.3' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer1_weights.V.1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer1_weights.V.1.0' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer1_weights.V.1.1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer1_weights.V.1.2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer1_weights.V.1.3' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer1_weights.V.2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer1_weights.V.2.0' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer1_weights.V.2.1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer1_weights.V.2.2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer1_weights.V.2.3' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer1_weights.V.3' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer1_weights.V.3.0' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer1_weights.V.3.1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer1_weights.V.3.2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'conv_layer1_weights.V.3.3' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'relu' into 'conv_layer1' (nnet_unroll/solution1/nnet.cpp:29) automatically.
INFO: [XFORM 203-602] Inlining function 'relu' into 'conv_layer2' (nnet_unroll/solution1/nnet.cpp:52) automatically.
INFO: [XFORM 203-602] Inlining function 'relu' into 'fc_layer1' (nnet_unroll/solution1/nnet.cpp:117) automatically.
INFO: [XFORM 203-602] Inlining function 'relu' into 'fc_layer2' (nnet_unroll/solution1/nnet.cpp:131) automatically.
INFO: [XFORM 203-602] Inlining function 'relu' into 'fc_layer3' (nnet_unroll/solution1/nnet.cpp:145) automatically.
INFO: [XFORM 203-602] Inlining function 'fc_layer1' into 'nnet' (nnet_unroll/solution1/nnet.cpp:161) automatically.
INFO: [XFORM 203-602] Inlining function 'fc_layer2' into 'nnet' (nnet_unroll/solution1/nnet.cpp:162) automatically.
INFO: [XFORM 203-602] Inlining function 'fc_layer3' into 'nnet' (nnet_unroll/solution1/nnet.cpp:163) automatically.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:04:13 ; elapsed = 00:03:25 . Memory (MB): peak = 827.062 ; gain = 479.004 ; free physical = 328 ; free virtual = 3008
                         Cannot flatten a loop nest 'Loop-1' (nnet_unroll/solution1/nnet.cpp:82:15) in function 'pool_layer2' : 

the outer loop is not a perfect loop.
                         Cannot flatten a loop nest 'Loop-1' (nnet_unroll/solution1/nnet.cpp:62:15) in function 'pool_layer1' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1.1' (nnet_unroll/solution1/nnet.cpp:42:7) in function 'conv_layer2'.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (nnet_unroll/solution1/nnet.cpp:41:6) in function 'conv_layer2'.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1.1' (nnet_unroll/solution1/nnet.cpp:19:7) in function 'conv_layer1'.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (nnet_unroll/solution1/nnet.cpp:18:6) in function 'conv_layer1'.
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:04:20 ; elapsed = 00:03:32 . Memory (MB): peak = 869.055 ; gain = 520.996 ; free physical = 231 ; free virtual = 2954
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'nnet' ...
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'conv_layer1'
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'L_L_conv_layer1_label3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 13.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 213.63 seconds; current allocated memory: 332.539 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 25.99 seconds; current allocated memory: 333.796 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'pool_layer1'
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'pool_layer1_label11'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('image_V_load_159', nnet_unroll/solution1/nnet.cpp:66) on array 'image_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'image_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 35, Depth = 36.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 28.44 seconds; current allocated memory: 335.332 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 44.56 seconds; current allocated memory: 337.079 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'conv_layer2'
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'L_L_conv_layer2_label5'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('image_V_load_215', nnet_unroll/solution1/nnet.cpp:51) on array 'image_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'image_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 16, Depth = 21.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 47.55 seconds; current allocated memory: 339.054 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 54.23 seconds; current allocated memory: 341.292 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'pool_layer2'
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'pool_layer2_label15'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('image_V_load_17', nnet_unroll/solution1/nnet.cpp:86) on array 'image_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'image_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 71, Depth = 72.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 61.74 seconds; current allocated memory: 343.952 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 80.69 seconds; current allocated memory: 347.144 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'nnet'
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 81.72 seconds; current allocated memory: 347.916 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 16.54 seconds; current allocated memory: 349.096 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'conv_layer1'
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'conv_layer1_conv_layer1_weights_31' to 'conv_layer1_conv_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer1_conv_layer1_weights_29' to 'conv_layer1_conv_cud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer1_conv_layer1_weights_27' to 'conv_layer1_conv_dEe' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer1_conv_layer1_weights_25' to 'conv_layer1_conv_eOg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer1_conv_layer1_weights_23' to 'conv_layer1_conv_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer1_conv_layer1_weights_21' to 'conv_layer1_conv_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer1_conv_layer1_weights_19' to 'conv_layer1_conv_hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer1_conv_layer1_weights_17' to 'conv_layer1_conv_ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer1_conv_layer1_weights_15' to 'conv_layer1_conv_jbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer1_conv_layer1_weights_13' to 'conv_layer1_conv_kbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer1_conv_layer1_weights_11' to 'conv_layer1_conv_lbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer1_conv_layer1_weights_9' to 'conv_layer1_conv_mb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer1_conv_layer1_weights_7' to 'conv_layer1_conv_ncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer1_conv_layer1_weights_5' to 'conv_layer1_conv_ocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer1_conv_layer1_weights_3' to 'conv_layer1_conv_pcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer1_conv_layer1_weights_1' to 'conv_layer1_conv_qcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer1_conv_layer1_bias_V' to 'conv_layer1_conv_rcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'nnet_mul_mul_19s_20ns_39_1_1' to 'nnet_mul_mul_19s_sc4' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'nnet_mul_mul_19s_sc4': 16 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'conv_layer1'.
INFO: [HLS 200-111]  Elapsed time: 17.21 seconds; current allocated memory: 351.876 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'pool_layer1'
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'pool_layer1'.
INFO: [HLS 200-111]  Elapsed time: 27.69 seconds; current allocated memory: 359.269 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'conv_layer2'
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_63' to 'conv_layer2_conv_tde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_61' to 'conv_layer2_conv_udo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_59' to 'conv_layer2_conv_vdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_57' to 'conv_layer2_conv_wdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_55' to 'conv_layer2_conv_xdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_53' to 'conv_layer2_conv_yd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_51' to 'conv_layer2_conv_zec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_49' to 'conv_layer2_conv_Aem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_47' to 'conv_layer2_conv_Bew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_45' to 'conv_layer2_conv_CeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_43' to 'conv_layer2_conv_DeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_41' to 'conv_layer2_conv_Ee0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_39' to 'conv_layer2_conv_Ffa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_37' to 'conv_layer2_conv_Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_35' to 'conv_layer2_conv_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_33' to 'conv_layer2_conv_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_31' to 'conv_layer2_conv_JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_29' to 'conv_layer2_conv_KfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_27' to 'conv_layer2_conv_Lf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_25' to 'conv_layer2_conv_Mgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_23' to 'conv_layer2_conv_Ngs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_21' to 'conv_layer2_conv_OgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_19' to 'conv_layer2_conv_PgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_17' to 'conv_layer2_conv_QgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_15' to 'conv_layer2_conv_Rg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_13' to 'conv_layer2_conv_Shg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_11' to 'conv_layer2_conv_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_9' to 'conv_layer2_conv_UhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_7' to 'conv_layer2_conv_VhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_5' to 'conv_layer2_conv_WhU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_3' to 'conv_layer2_conv_Xh4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_weights_1' to 'conv_layer2_conv_Yie' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'conv_layer2_conv_layer2_bias_V' to 'conv_layer2_conv_Zio' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'nnet_mul_mul_19s_24s_43_1_1' to 'nnet_mul_mul_19s_0iy' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'nnet_mul_mul_19s_0iy': 32 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'conv_layer2'.
INFO: [HLS 200-111]  Elapsed time: 52.05 seconds; current allocated memory: 369.764 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'pool_layer2'
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'pool_layer2'.
INFO: [HLS 200-111]  Elapsed time: 61.34 seconds; current allocated memory: 383.479 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'nnet'
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'nnet/conv_layer1_out_V' to 'ap_memory'.
INFO: [RTGEN 206-500] Setting interface mode on port 'nnet/conv_layer2_out_V' to 'ap_memory'.
INFO: [RTGEN 206-500] Setting interface mode on port 'nnet/pool_layer1_out_V' to 'ap_memory'.
INFO: [RTGEN 206-500] Setting interface mode on port 'nnet/pool_layer2_out_V' to 'ap_memory'.
INFO: [RTGEN 206-500] Setting interface mode on port 'nnet/fc_layer1_out_V' to 'ap_memory'.
INFO: [RTGEN 206-500] Setting interface mode on port 'nnet/fc_layer2_out_V' to 'ap_memory'.
INFO: [RTGEN 206-500] Setting interface mode on port 'nnet/fc_layer3_out_V' to 'ap_memory'.
INFO: [RTGEN 206-500] Setting interface mode on function 'nnet' to 'ap_ctrl_hs'.
WARNING: [RTGEN 206-101] Global array 'image_V' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer1_weights_31' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer1_weights_29' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer1_weights_27' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer1_weights_25' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer1_weights_23' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer1_weights_21' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer1_weights_19' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer1_weights_17' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer1_weights_15' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer1_weights_13' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer1_weights_11' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer1_weights_9' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer1_weights_7' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer1_weights_5' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer1_weights_3' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer1_weights_1' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer1_bias_V' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_63' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_61' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_59' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_57' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_55' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_53' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_51' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_49' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_47' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_45' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_43' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_41' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_39' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_37' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_35' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_33' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_31' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_29' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_27' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_25' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_23' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_21' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_19' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_17' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_15' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_13' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_11' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_9' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_7' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_5' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_3' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_weights_1' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'conv_layer2_bias_V' will not be exposed as RTL port.
WARNING: [RTGEN 206-101] Global array 'fc_layer1_weights_V' will not be exposed as RTL port.
INFO: [SYN 201-210] Renamed object name 'nnet_fc_layer1_weights_V' to 'nnet_fc_layer1_we1iI' due to the length limit 20
WARNING: [RTGEN 206-101] Global array 'fc_layer2_weights_V' will not be exposed as RTL port.
INFO: [SYN 201-210] Renamed object name 'nnet_fc_layer2_weights_V' to 'nnet_fc_layer2_we2iS' due to the length limit 20
WARNING: [RTGEN 206-101] Global array 'fc_layer3_weights_V' will not be exposed as RTL port.
INFO: [SYN 201-210] Renamed object name 'nnet_fc_layer3_weights_V' to 'nnet_fc_layer3_we3i2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'nnet_mul_mul_24s_18s_41_1_1' to 'nnet_mul_mul_24s_4jc' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'nnet_mul_mul_24s_19s_42_1_1' to 'nnet_mul_mul_24s_5jm' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'nnet_mul_mul_24s_4jc': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'nnet_mul_mul_24s_5jm': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'nnet'.
INFO: [HLS 200-111]  Elapsed time: 102.92 seconds; current allocated memory: 396.562 MB.
INFO: [RTMG 210-279] Implementing memory 'conv_layer1_image_V_rom' using block ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer1_conv_bkb_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer1_conv_cud_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer1_conv_dEe_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer1_conv_eOg_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer1_conv_fYi_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer1_conv_g8j_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer1_conv_hbi_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer1_conv_ibs_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer1_conv_jbC_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer1_conv_kbM_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer1_conv_lbW_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer1_conv_mb6_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer1_conv_ncg_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer1_conv_ocq_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer1_conv_pcA_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer1_conv_qcK_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer1_conv_rcU_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_tde_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_udo_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_vdy_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_wdI_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_xdS_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_yd2_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_zec_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_Aem_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_Bew_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_CeG_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_DeQ_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_Ee0_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_Ffa_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_Gfk_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_Hfu_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_IfE_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_JfO_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_KfY_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_Lf8_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_Mgi_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_Ngs_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_OgC_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_PgM_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_QgW_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_Rg6_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_Shg_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_Thq_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_UhA_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_VhK_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_WhU_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_Xh4_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_Yie_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'conv_layer2_conv_Zio_rom' using distributed ROMs.
INFO: [RTMG 210-279] Implementing memory 'nnet_fc_layer1_we1iI_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'nnet_fc_layer2_we2iS_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'nnet_fc_layer3_we3i2_rom' using auto ROMs.
INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:16:36 ; elapsed = 00:16:05 . Memory (MB): peak = 869.055 ; gain = 520.996 ; free physical = 137 ; free virtual = 2923
INFO: [SYSC 207-301] Generating SystemC RTL for nnet.
INFO: [VHDL 208-304] Generating VHDL RTL for nnet.
INFO: [VLOG 209-307] Generating Verilog RTL for nnet.
INFO: [HLS 200-112] Total elapsed time: 965.77 seconds; peak allocated memory: 396.562 MB.
